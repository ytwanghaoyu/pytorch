{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b537a4f",
   "metadata": {},
   "source": [
    "## 1. 导入依赖并设置随机种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "988cda86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11009b570>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import onnx\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5640be1",
   "metadata": {},
   "source": [
    "## 2. 定义实验模型\n",
    "\n",
    "我们创建一个简单的模型，包含两个变量：\n",
    "- `my_tensor`: 普通的 Tensor（即使设置了 `requires_grad=True`）\n",
    "- `my_param`: nn.Parameter 包装的 Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60322bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型结构:\n",
      "ExperimentModel()\n"
     ]
    }
   ],
   "source": [
    "class ExperimentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 1. 定义一个普通的 Tensor\n",
    "        # 即使我们手动设置 requires_grad=True，它也只是一个普通的张量\n",
    "        # 为了演示效果，我们先设置为 False\n",
    "        self.my_tensor = torch.randn(1, requires_grad=False)\n",
    "\n",
    "        # 2. 定义一个 nn.Parameter\n",
    "        # nn.Parameter 本质上也是 Tensor，但会自动注册到模型的 parameters() 中\n",
    "        self.my_param = nn.Parameter(torch.randn(1))\n",
    "\n",
    "        # 3. 注册一个缓冲区 (buffer)\n",
    "        # 缓冲区不会被视为模型参数，但会随着模型一起保存和加载\n",
    "        self.register_buffer('my_buffer', torch.randn(1))\n",
    "\n",
    "        # 4. 注册一个持久化状态 (persistent state)\n",
    "        # 这不是 Tensor，而是一个普通的 Python 对象\n",
    "        self.register_buffer('my_state', torch.tensor(42), persistent=False)\n",
    "\n",
    "        # 5. 注册一个参数，和 nn.Parameter 类似，但使用 register_parameter 方法\n",
    "        self.register_parameter('my_registered_param', nn.Parameter(torch.randn(1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 简单的线性变换: y = x * tensor + param\n",
    "        return x * self.my_tensor + self.my_param\n",
    "\n",
    "model = ExperimentModel()\n",
    "print(\"模型结构:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d27834",
   "metadata": {},
   "source": [
    "## 3. 查看模型参数\n",
    "\n",
    "关键观察：只有 `nn.Parameter` 会被注册到模型的参数列表中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff44752a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 模型眼中的参数 (model.named_parameters) ===\n",
      "参数名: my_param\n",
      "  - 值: tensor([-1.6898])\n",
      "  - 类型: <class 'torch.nn.parameter.Parameter'>\n",
      "  - requires_grad: True\n",
      "参数名: my_registered_param\n",
      "  - 值: tensor([0.9580])\n",
      "  - 类型: <class 'torch.nn.parameter.Parameter'>\n",
      "  - requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "print(\"=== 模型眼中的参数 (model.named_parameters) ===\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"参数名: {name}\")\n",
    "    print(f\"  - 值: {param.data}\")\n",
    "    print(f\"  - 类型: {type(param)}\")\n",
    "    print(f\"  - requires_grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15900e65",
   "metadata": {},
   "source": [
    "## 4. 训练实验：观察更新行为\n",
    "\n",
    "我们进行一次简单的训练步骤，观察哪些变量会被优化器更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e54bfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 训练前的值 ===\n",
      "my_tensor: 1.1103\n",
      "my_param:  -1.6898\n"
     ]
    }
   ],
   "source": [
    "# 创建优化器（只会优化 model.parameters() 中的变量）\n",
    "optimizer = optim.SGD(model.parameters(), lr=1.0)\n",
    "\n",
    "# 准备数据\n",
    "data = torch.tensor([1.0])\n",
    "target = torch.tensor([0.0])\n",
    "\n",
    "# 记录初始值\n",
    "init_tensor_val = model.my_tensor.item()\n",
    "init_param_val = model.my_param.item()\n",
    "\n",
    "print(\"=== 训练前的值 ===\")\n",
    "print(f\"my_tensor: {init_tensor_val:.4f}\")\n",
    "print(f\"my_param:  {init_param_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c53ebc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "模型输出: -0.5795\n",
      "损失值: 0.3358\n",
      "\n",
      "=== 梯度信息 ===\n",
      "my_tensor.grad: None\n",
      "my_param.grad:  tensor([-1.1590])\n"
     ]
    }
   ],
   "source": [
    "# 前向传播\n",
    "output = model(data)\n",
    "print(f\"\\n模型输出: {output.item():.4f}\")\n",
    "\n",
    "# 计算损失\n",
    "loss = (output - target) ** 2\n",
    "print(f\"损失值: {loss.item():.4f}\")\n",
    "\n",
    "# 反向传播\n",
    "loss.backward()\n",
    "\n",
    "# 查看梯度\n",
    "print(\"\\n=== 梯度信息 ===\")\n",
    "print(f\"my_tensor.grad: {model.my_tensor.grad}\")\n",
    "print(f\"my_param.grad:  {model.my_param.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "744ea708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 训练一步后的变化 ===\n",
      "my_tensor: 1.1103 -> 1.1103 (变化: 0.0000)\n",
      "my_param:  -1.6898 -> -0.5308 (变化: 1.1590)\n",
      "\n",
      "结论: 只有 nn.Parameter 被优化器更新了！\n"
     ]
    }
   ],
   "source": [
    "# 优化器更新\n",
    "optimizer.step()\n",
    "\n",
    "print(\"\\n=== 训练一步后的变化 ===\")\n",
    "print(f\"my_tensor: {init_tensor_val:.4f} -> {model.my_tensor.item():.4f} (变化: {model.my_tensor.item() - init_tensor_val:.4f})\")\n",
    "print(f\"my_param:  {init_param_val:.4f} -> {model.my_param.item():.4f} (变化: {model.my_param.item() - init_param_val:.4f})\")\n",
    "\n",
    "print(\"\\n结论: 只有 nn.Parameter 被优化器更新了！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa0266f",
   "metadata": {},
   "source": [
    "## 5. 使用 requires_grad=True 的 Tensor\n",
    "\n",
    "即使我们给普通 Tensor 设置 `requires_grad=True`，它仍然不会被优化器更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bcf943b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== requires_grad=True 的 Tensor ===\n",
      "my_tensor.requires_grad: True\n",
      "my_param.requires_grad:  True\n",
      "\n",
      "=== 模型参数列表 ===\n",
      "参数数量: 1\n",
      "  - my_param\n",
      "\n",
      "结论: 即使 requires_grad=True，普通 Tensor 也不会成为模型参数！\n"
     ]
    }
   ],
   "source": [
    "class ExperimentModel2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 这次设置 requires_grad=True\n",
    "        self.my_tensor = torch.randn(1, requires_grad=True)\n",
    "        self.my_param = nn.Parameter(torch.randn(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.my_tensor + self.my_param\n",
    "\n",
    "model2 = ExperimentModel2()\n",
    "\n",
    "print(\"=== requires_grad=True 的 Tensor ===\")\n",
    "print(f\"my_tensor.requires_grad: {model2.my_tensor.requires_grad}\")\n",
    "print(f\"my_param.requires_grad:  {model2.my_param.requires_grad}\")\n",
    "\n",
    "print(\"\\n=== 模型参数列表 ===\")\n",
    "print(f\"参数数量: {len(list(model2.parameters()))}\")\n",
    "for name, param in model2.named_parameters():\n",
    "    print(f\"  - {name}\")\n",
    "\n",
    "print(\"\\n结论: 即使 requires_grad=True，普通 Tensor 也不会成为模型参数！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6531a5",
   "metadata": {},
   "source": [
    "## 6. ONNX 导出分析\n",
    "\n",
    "在导出为 ONNX 格式时，只有 `nn.Parameter` 会被保存为模型的初始化器（initializer）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcb943ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ONNX 导出分析 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1130 12:32:04.969000 59267 /Volumes/BitTopia/PyTorch/pytorch/torch/onnx/_internal/exporter/_registration.py:110] torchvision is not installed. Skipping torchvision::nms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `ExperimentModel()` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `ExperimentModel()` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "模型已导出到: /tmp/experiment_model.onnx\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "模型已导出到: /tmp/experiment_model.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/howiewang/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/copyreg.py:99: FutureWarning: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.\n",
      "  return cls.__new__(cls, *args)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== ONNX 导出分析 ===\")\n",
    "onnx_path = \"/tmp/experiment_model.onnx\"\n",
    "\n",
    "# 导出模型\n",
    "model.eval()\n",
    "dummy_input = torch.tensor([1.0])\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (dummy_input,),\n",
    "    onnx_path,\n",
    "    input_names=[\"input_x\"],\n",
    "    output_names=[\"output\"],\n",
    "    opset_version=18,\n",
    ")\n",
    "\n",
    "print(f\"模型已导出到: {onnx_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b64692c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ONNX 图的 initializer (模型参数) ---\n",
      "\n",
      "名字: my_param\n",
      "  - 值: [-0.8021901]\n",
      "  - 形状: [1]\n",
      "  - 数据类型: 1\n",
      "\n",
      "名字: my_tensor\n",
      "  - 值: [0.33669037]\n",
      "  - 形状: [1]\n",
      "  - 数据类型: 1\n"
     ]
    }
   ],
   "source": [
    "# 分析 ONNX 结构\n",
    "model_onnx = onnx.load(onnx_path)\n",
    "graph = model_onnx.graph\n",
    "\n",
    "print(\"\\n--- ONNX 图的 initializer (模型参数) ---\")\n",
    "if len(graph.initializer) == 0:\n",
    "    print(\"  (无 initializer)\")\n",
    "else:\n",
    "    for initializer in graph.initializer:\n",
    "        arr = onnx.numpy_helper.to_array(initializer)\n",
    "        print(f\"\\n名字: {initializer.name}\")\n",
    "        print(f\"  - 值: {arr}\")\n",
    "        print(f\"  - 形状: {initializer.dims}\")\n",
    "        print(f\"  - 数据类型: {initializer.data_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b454959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ONNX 图的节点 (计算图) ---\n",
      "\n",
      "节点 0: Mul\n",
      "  - 输入: ['input_x', 'my_tensor']\n",
      "  - 输出: ['mul']\n",
      "\n",
      "节点 1: Add\n",
      "  - 输入: ['mul', 'my_param']\n",
      "  - 输出: ['output']\n"
     ]
    }
   ],
   "source": [
    "# 分析 ONNX 节点\n",
    "print(\"\\n--- ONNX 图的节点 (计算图) ---\")\n",
    "for i, node in enumerate(graph.node):\n",
    "    print(f\"\\n节点 {i}: {node.op_type}\")\n",
    "    print(f\"  - 输入: {list(node.input)}\")\n",
    "    print(f\"  - 输出: {list(node.output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edd51e8",
   "metadata": {},
   "source": [
    "### ONNX 导出观察\n",
    "\n",
    "可以看到：\n",
    "- `my_param` 被保存为 ONNX 的 initializer（模型的可学习参数）\n",
    "- `my_tensor` 被当作常量直接嵌入到计算图中（或作为常量节点）\n",
    "\n",
    "这意味着在部署时：\n",
    "- Parameter 的值可以被轻松替换（如加载新的权重）\n",
    "- Tensor 的值被硬编码到模型中，难以修改"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dc34a2",
   "metadata": {},
   "source": [
    "## 7. 总结\n",
    "\n",
    "| 特性 | 普通 Tensor | nn.Parameter |\n",
    "|------|------------|-------------|\n",
    "| **本质** | torch.Tensor | torch.Tensor（子类） |\n",
    "| **requires_grad** | 需要手动设置 | 自动设置为 True |\n",
    "| **注册到 model.parameters()** | ❌ 不会 | ✅ 自动注册 |\n",
    "| **被优化器更新** | ❌ 不会 | ✅ 会被更新 |\n",
    "| **保存/加载 state_dict** | ❌ 不会 | ✅ 会被保存 |\n",
    "| **ONNX 导出** | 常量/硬编码 | initializer（参数） |\n",
    "| **适用场景** | 固定值、中间变量 | 可学习的权重/偏置 |\n",
    "\n",
    "### 关键要点\n",
    "\n",
    "1. **nn.Parameter 是特殊的 Tensor**\n",
    "   - 自动注册到模型的参数列表\n",
    "   - 可被优化器自动发现和更新\n",
    "\n",
    "2. **普通 Tensor 不是模型参数**\n",
    "   - 即使设置 `requires_grad=True`，也不会被优化器更新\n",
    "   - 适合存储常量或不需要训练的值\n",
    "\n",
    "3. **在模型导出时的差异**\n",
    "   - Parameter: 作为独立的权重被保存\n",
    "   - Tensor: 作为常量嵌入计算图\n",
    "\n",
    "4. **最佳实践**\n",
    "   - 可学习的权重 → 使用 `nn.Parameter`\n",
    "   - 固定常量 → 使用普通 `Tensor`\n",
    "   - 中间计算结果 → 使用普通 `Tensor`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db3a22a",
   "metadata": {},
   "source": [
    "## 8. 实战示例：何时使用 nn.Parameter\n",
    "\n",
    "让我们看一个实际的例子：自定义的带偏置的线性层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71f214c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自定义线性层的参数:\n",
      "  - weight: shape torch.Size([2, 3])\n",
      "  - bias: shape torch.Size([2])\n",
      "\n",
      "注意: scale 不在参数列表中（这是我们想要的效果）\n"
     ]
    }
   ],
   "source": [
    "class CustomLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        # 权重矩阵 - 需要训练\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        # 偏置向量 - 需要训练\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "        # 缩放因子 - 固定值，不需要训练\n",
    "        self.scale = torch.tensor(0.1)  # 普通 Tensor\n",
    "\n",
    "    def forward(self, x):\n",
    "        # y = (xW^T + b) * scale\n",
    "        return (x @ self.weight.t() + self.bias) * self.scale\n",
    "\n",
    "linear = CustomLinear(3, 2)\n",
    "\n",
    "print(\"自定义线性层的参数:\")\n",
    "for name, param in linear.named_parameters():\n",
    "    print(f\"  - {name}: shape {param.shape}\")\n",
    "\n",
    "print(\"\\n注意: scale 不在参数列表中（这是我们想要的效果）\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
