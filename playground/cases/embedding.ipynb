{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a299ef4",
   "metadata": {},
   "source": [
    "## 1. 导入依赖并设置随机种子\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e98be26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10e29b570>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d533a62f",
   "metadata": {},
   "source": [
    "## 2. 创建 Embedding 层\n",
    "\n",
    "假设我们有一个小型词表（5 个词），每个词用 3 维向量表示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c7e3951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 层的权重形状: torch.Size([5, 3])\n",
      "Embedding 层的权重类型: <class 'torch.nn.parameter.Parameter'>\n",
      "\n",
      "Embedding 权重矩阵:\n",
      "Parameter containing:\n",
      "tensor([[ 0.3367,  0.1288,  0.2345],\n",
      "        [ 0.2303, -1.1229, -0.1863],\n",
      "        [ 2.2082, -0.6380,  0.4617],\n",
      "        [ 0.2674,  0.5349,  0.8094],\n",
      "        [ 1.1103, -1.6898, -0.9890]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 假设词表大小为 5，每个词向量维度为 3\n",
    "num_embeddings = 5\n",
    "embedding_dim = 3\n",
    "\n",
    "# 创建一个 Embedding 层\n",
    "emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "print(f\"Embedding 层的权重形状: {emb_layer.weight.shape}\")\n",
    "print(f\"Embedding 层的权重类型: {type(emb_layer.weight)}\")\n",
    "print(f\"\\nEmbedding 权重矩阵:\\n{emb_layer.weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d896fb0",
   "metadata": {},
   "source": [
    "## 3. 比较两种实现方式\n",
    "\n",
    "我们将比较两种获取词向量的方法：\n",
    "\n",
    "- **方法 A**: 使用 `nn.Embedding` 直接查表\n",
    "- **方法 B**: 使用 One-Hot 编码 + 矩阵乘法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "419829c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方法 A (Embedding 查表) 输出:\n",
      "tensor([[ 0.2303, -1.1229, -0.1863],\n",
      "        [ 0.2674,  0.5349,  0.8094]], grad_fn=<EmbeddingBackward0>)\n",
      "grad_fn: <EmbeddingBackward0 object at 0x111ecf0a0>\n"
     ]
    }
   ],
   "source": [
    "# 输入：获取索引为 1 和 3 的词向量\n",
    "input_indices = torch.tensor([1, 3])\n",
    "\n",
    "# --- 方法 A: 使用 nn.Embedding (查表) ---\n",
    "output_emb = emb_layer(input_indices)\n",
    "print(\"方法 A (Embedding 查表) 输出:\")\n",
    "print(output_emb)\n",
    "print(f\"grad_fn: {output_emb.grad_fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a2bd1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot 编码:\n",
      "tensor([[0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.]])\n",
      "\n",
      "方法 B (One-Hot + MatMul) 输出:\n",
      "tensor([[ 0.2303, -1.1229, -0.1863],\n",
      "        [ 0.2674,  0.5349,  0.8094]], grad_fn=<MmBackward0>)\n",
      "grad_fn: <MmBackward0 object at 0x1116aca30>\n"
     ]
    }
   ],
   "source": [
    "# --- 方法 B: 手动模拟 (One-Hot + 矩阵乘法) ---\n",
    "one_hot = F.one_hot(input_indices, num_classes=num_embeddings).float()\n",
    "print(\"One-Hot 编码:\")\n",
    "print(one_hot)\n",
    "\n",
    "output_matmul = torch.matmul(one_hot, emb_layer.weight)\n",
    "print(\"\\n方法 B (One-Hot + MatMul) 输出:\")\n",
    "print(output_matmul)\n",
    "print(f\"grad_fn: {output_matmul.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816faafe",
   "metadata": {},
   "source": [
    "### 验证数学等价性\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08dd250b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "两者结果是否数学等价? True\n",
      "\n",
      "最大差异: 0.0000000000\n"
     ]
    }
   ],
   "source": [
    "# 验证两者是否完全相等\n",
    "are_equal = torch.allclose(output_emb, output_matmul)\n",
    "print(f\"两者结果是否数学等价? {are_equal}\")\n",
    "print(f\"\\n最大差异: {torch.max(torch.abs(output_emb - output_matmul)).item():.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb94b103",
   "metadata": {},
   "source": [
    "## 4. 反向传播的差异\n",
    "\n",
    "虽然前向传播结果相同，但是 `grad_fn` 不同：\n",
    "\n",
    "- `EmbeddingBackward0`: Embedding 查表的反向传播\n",
    "- `MmBackward0`: 矩阵乘法的反向传播\n",
    "\n",
    "让我们验证梯度是否相同：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f322157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始权重矩阵:\n",
      "Parameter containing:\n",
      "tensor([[ 0.9580,  1.3221,  0.8172],\n",
      "        [-0.7658, -0.7506,  1.3525],\n",
      "        [ 0.6863, -0.3278,  0.7950],\n",
      "        [ 0.2815,  0.0562,  0.5227],\n",
      "        [-0.2384, -0.0499,  0.5263]], requires_grad=True)\n",
      "\n",
      "从上游传来的梯度:\n",
      "tensor([[-0.0085,  0.7291,  0.1331],\n",
      "        [ 0.8640, -1.0157, -0.8887]])\n"
     ]
    }
   ],
   "source": [
    "# 准备两个相同的权重矩阵\n",
    "weight_data = torch.randn(5, 3)\n",
    "W_emb = torch.nn.Parameter(weight_data.clone())\n",
    "W_matmul = torch.nn.Parameter(weight_data.clone())\n",
    "\n",
    "# 输入 ID\n",
    "indices = torch.tensor([1, 3])\n",
    "grad_output = torch.randn(2, 3)  # 假设传回来的梯度\n",
    "\n",
    "print(\"初始权重矩阵:\")\n",
    "print(W_emb)\n",
    "print(\"\\n从上游传来的梯度:\")\n",
    "print(grad_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25633a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 的梯度 (W_emb.grad):\n",
      "tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.0085,  0.7291,  0.1331],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.8640, -1.0157, -0.8887],\n",
      "        [ 0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# --- 路径 A: Embedding ---\n",
    "out_a = F.embedding(indices, W_emb)\n",
    "out_a.backward(grad_output)\n",
    "\n",
    "print(\"Embedding 的梯度 (W_emb.grad):\")\n",
    "print(W_emb.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5cc883e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MatMul 的梯度 (W_matmul.grad):\n",
      "tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.0085,  0.7291,  0.1331],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.8640, -1.0157, -0.8887],\n",
      "        [ 0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# --- 路径 B: MatMul ---\n",
    "one_hot = F.one_hot(indices, num_classes=5).float()\n",
    "out_b = torch.matmul(one_hot, W_matmul)\n",
    "out_b.backward(grad_output)\n",
    "\n",
    "print(\"MatMul 的梯度 (W_matmul.grad):\")\n",
    "print(W_matmul.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "717be718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "梯度数值是否完全相等? True\n",
      "\n",
      "梯度最大差异: 0.0000000000\n"
     ]
    }
   ],
   "source": [
    "# 验证梯度是否相等\n",
    "print(f\"梯度数值是否完全相等? {torch.allclose(W_emb.grad, W_matmul.grad)}\")\n",
    "print(f\"\\n梯度最大差异: {torch.max(torch.abs(W_emb.grad - W_matmul.grad)).item():.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d741bc4",
   "metadata": {},
   "source": [
    "## 5. 为什么需要 EmbeddingBackward？\n",
    "\n",
    "既然结果一样，为什么 PyTorch 要专门设计 `EmbeddingBackward` 而不直接用矩阵乘法？\n",
    "\n",
    "### 原因 1: 计算效率\n",
    "\n",
    "- Embedding: 直接索引，时间复杂度 O(1)\n",
    "- MatMul: 需要先构造 One-Hot 矩阵，再进行矩阵乘法，时间复杂度 O(vocab_size)\n",
    "\n",
    "### 原因 2: 稀疏梯度（Sparse Gradients）\n",
    "\n",
    "这是最重要的原因！让我们演示一下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a519839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 稠密梯度 (Dense) vs 稀疏梯度 (Sparse) ===\n",
      "\n",
      "MatMul 产生的梯度 (稠密):\n",
      "  - 形状: torch.Size([5, 3])\n",
      "  - 非零元素数量: 6\n",
      "  - 内存占用: 60 字节\n",
      "\n",
      "Embedding 产生的梯度 (也是稠密的，但可以配置为稀疏):\n",
      "  - 形状: torch.Size([5, 3])\n",
      "  - 非零元素数量: 6\n",
      "  - 内存占用: 60 字节\n"
     ]
    }
   ],
   "source": [
    "# 对比稠密梯度和稀疏梯度\n",
    "print(\"=== 稠密梯度 (Dense) vs 稀疏梯度 (Sparse) ===\")\n",
    "print(\"\\nMatMul 产生的梯度 (稠密):\")\n",
    "print(f\"  - 形状: {W_matmul.grad.shape}\")\n",
    "print(f\"  - 非零元素数量: {torch.count_nonzero(W_matmul.grad)}\")\n",
    "print(f\"  - 内存占用: {W_matmul.grad.element_size() * W_matmul.grad.nelement()} 字节\")\n",
    "\n",
    "print(\"\\nEmbedding 产生的梯度 (也是稠密的，但可以配置为稀疏):\")\n",
    "print(f\"  - 形状: {W_emb.grad.shape}\")\n",
    "print(f\"  - 非零元素数量: {torch.count_nonzero(W_emb.grad)}\")\n",
    "print(f\"  - 内存占用: {W_emb.grad.element_size() * W_emb.grad.nelement()} 字节\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ef5364",
   "metadata": {},
   "source": [
    "### 稀疏梯度的重要性\n",
    "\n",
    "当使用 `nn.Embedding(sparse=True)` 时：\n",
    "\n",
    "- **MatMul 的梯度**：稠密张量（Dense Tensor），即使只更新了 2 个词，也会生成一个和整个词表一样大的矩阵（比如 10 万行），其中 99,998 行都是 0。这非常占显存。\n",
    "- **Embedding 的梯度**：稀疏张量（Sparse Tensor），只记录 `(index=1, value=...), (index=3, value=...)`，不存储那些 0。\n",
    "\n",
    "**对于超大词表（如几百万词）的训练，稀疏梯度至关重要！**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5a5cf7",
   "metadata": {},
   "source": [
    "## 6. 总结\n",
    "\n",
    "| 特性           | Embedding 查表       | One-Hot + MatMul       |\n",
    "| -------------- | -------------------- | ---------------------- |\n",
    "| **前向计算**   | O(1) 直接索引        | O(vocab_size) 矩阵运算 |\n",
    "| **反向传播**   | `EmbeddingBackward0` | `MmBackward0`          |\n",
    "| **梯度类型**   | 可配置稀疏梯度       | 始终是稠密梯度         |\n",
    "| **内存效率**   | 高（稀疏模式下）     | 低（大词表时）         |\n",
    "| **数学等价性** | ✅ 完全等价          | ✅ 完全等价            |\n",
    "| **适用场景**   | 大规模 NLP 任务      | 教学演示               |\n",
    "\n",
    "### 关键要点\n",
    "\n",
    "1. **数学上完全等价**：Embedding 查表 = One-Hot 编码 + 矩阵乘法\n",
    "2. **实现上大不相同**：Embedding 专门优化了索引操作和稀疏梯度\n",
    "3. **工程上的选择**：对于大词表，使用 `nn.Embedding(sparse=True)` 可以节省大量内存\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f3ef3b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
