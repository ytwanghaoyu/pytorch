{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2959d1c8",
   "metadata": {},
   "source": [
    "## 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42150254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0271e05",
   "metadata": {},
   "source": [
    "## 设置模型和训练参数\n",
    "\n",
    "模拟一个简单的训练循环，展示梯度累积的概念"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c4eead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(10, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# 假设我们有一个 DataLoader 提供训练数据\n",
    "dataloader = [\n",
    "    (torch.randn(4, 10), torch.randn(4, 1)) for _ in range(8)\n",
    "]  # 8 batches of size 4\n",
    "\n",
    "accumulation_steps = 2  # 每 2 个 batch 更新一次参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9eb12b",
   "metadata": {},
   "source": [
    "## 梯度累积示例\n",
    "\n",
    "演示如何在多个批次上累积梯度，然后一次性更新参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c421c49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Batch 1, weight.grad norm: 1.5881\n",
      "tensor([[-0.2413,  1.2419, -0.2858, -0.0980, -0.5272, -0.5415, -0.3746,  0.2173,\n",
      "          0.0060, -0.2678]])\n",
      "After Batch 2, weight.grad norm: 2.5128\n",
      "tensor([[ 0.0264,  1.6652, -0.7211, -0.3304, -0.2354, -0.3890, -1.2311,  0.6651,\n",
      "         -0.8374, -0.2127]])\n",
      "After Batch 3, weight.grad norm: 1.3218\n",
      "tensor([[-0.1533, -0.0759, -0.3419, -0.1852, -0.3860, -0.3199, -0.4549, -0.4195,\n",
      "         -0.8713, -0.4162]])\n",
      "After Batch 4, weight.grad norm: 1.9170\n",
      "tensor([[-0.8682,  1.0178, -0.9146, -0.5453, -0.1068, -0.6311, -0.1655, -0.3507,\n",
      "         -0.4149, -0.1385]])\n",
      "After Batch 5, weight.grad norm: 2.5140\n",
      "tensor([[ 0.9387,  1.4293, -0.1148, -0.7026, -0.1062,  1.2205,  0.2383, -0.9716,\n",
      "         -0.2133, -0.5845]])\n",
      "After Batch 6, weight.grad norm: 3.2805\n",
      "tensor([[ 0.8122,  1.9298,  0.5303, -0.8754, -0.7006,  0.6512, -0.7304, -1.0867,\n",
      "         -1.1870, -1.1368]])\n",
      "After Batch 7, weight.grad norm: 2.2507\n",
      "tensor([[-0.1511,  0.7296, -0.2645, -0.5020, -0.4527, -0.7587, -0.2668,  1.7211,\n",
      "          0.0276, -0.6116]])\n",
      "After Batch 8, weight.grad norm: 2.3387\n",
      "tensor([[ 0.4961,  0.8044,  0.1334,  0.1489, -0.9856, -0.4589, -0.3075,  1.4412,\n",
      "          0.4364, -0.9961]])\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()  # 1. 开始前清零\n",
    "\n",
    "for i, (inputs, labels) in enumerate(dataloader):\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # Loss 除以累积步数，以保持梯度量级一致\n",
    "    loss = loss / accumulation_steps\n",
    "    loss.backward()  # 2. 梯度累加到 .grad 中\n",
    "    print(\n",
    "        f\"After Batch {i + 1}, weight.grad norm: {model.weight.grad.norm().item():.4f}\"\n",
    "    )\n",
    "    print(model.weight.grad)  # 打印当前梯度值\n",
    "\n",
    "    if (i + 1) % accumulation_steps == 0:\n",
    "        optimizer.step()  # 3. 累积够了，更新一次参数\n",
    "        optimizer.zero_grad()  # 4. 更新完，立刻清零，为下一组做准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68878796",
   "metadata": {},
   "source": [
    "## 梯度累积的三个角色\n",
    "\n",
    "**loss.backward()：是 \"生产者\" (Producer)**\n",
    "\n",
    "它的工作是生产梯度。你可以生产一次，也可以生产多次（累积）。它是相对灵活的。\n",
    "\n",
    "**optimizer.step()：是 \"消费者\" (Consumer)**\n",
    "\n",
    "它的工作是消费梯度，修改参数。关键点：一旦它消费了当前的梯度并更新了参数，这些梯度瞬间就变成了\"垃圾\"（过期的历史数据）。\n",
    "\n",
    "**optimizer.zero_grad()：是 \"清洁工\" (Cleaner)**\n",
    "\n",
    "它的工作是清空梯度缓存。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
