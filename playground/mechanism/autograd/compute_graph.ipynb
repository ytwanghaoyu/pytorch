{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5dc4389",
   "metadata": {},
   "source": [
    "## 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e20f6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b5e658",
   "metadata": {},
   "source": [
    "## 场景 A: 动态控制流 (Define-by-Run)\n",
    "\n",
    "每次运行，for 循环次数可能不同，导致图结构完全不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60732d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, requires_grad=True)\n",
    "y = torch.randn(1, requires_grad=True)\n",
    "\n",
    "def dynamic_forward(x, y):\n",
    "    if x.item() > 0:\n",
    "        for _ in range(2):\n",
    "            y = y * x\n",
    "    else:\n",
    "        y = y + x\n",
    "    return y\n",
    "\n",
    "out = dynamic_forward(x, y)\n",
    "print(f\"Current Graph Root: {out.grad_fn}\")  # 可能是 MulBackward 或 AddBackward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e38617",
   "metadata": {},
   "source": [
    "## 场景 B: 图的销毁\n",
    "\n",
    "PyTorch 采用\"用完即弃\"的计算图管理策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be21dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = x * y\n",
    "loss.backward()  # 第一次反向传播，执行 apply() 并释放中间 Node 内存\n",
    "\n",
    "try:\n",
    "    loss.backward()  # 第二次尝试\n",
    "except RuntimeError as e:\n",
    "    print(f\"\\nError captured: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f59682",
   "metadata": {},
   "source": [
    "### 解释\n",
    "\n",
    "**1. 动态控制流 (Define-by-Run)：**\n",
    "\n",
    "PyTorch 在每次前向传播时动态构建计算图，因此可以根据输入数据的不同路径执行不同的计算。这使得模型可以灵活地处理各种输入情况，而不需要预定义所有可能的计算路径。\n",
    "\n",
    "**2. 图的销毁：**\n",
    "\n",
    "PyTorch 采用\"用完即弃\"的计算图管理策略。在调用 `backward()` 后，计算图中的中间节点会被释放以节省内存。如果尝试对同一个计算图调用 `backward()`，会因为图已经被销毁而抛出错误。这促使用户在需要多次反向传播时，显式地重新构建计算图。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbc656b",
   "metadata": {},
   "source": [
    "## 递归遍历计算图\n",
    "\n",
    "定义一个函数来递归遍历 grad_fn，文本化打印计算图结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22af4bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_autograd_graph(node, level=0):\n",
    "    \"\"\"\n",
    "    递归遍历 grad_fn，文本化打印计算图结构\n",
    "    \"\"\"\n",
    "    indent = \"    \" * level\n",
    "    # 打印当前节点类型\n",
    "    print(f\"{indent} -> {node}\")\n",
    "\n",
    "    # 检查是否有后续节点\n",
    "    if hasattr(node, \"next_functions\"):\n",
    "        for next_node, _ in node.next_functions:\n",
    "            # AccumulateGrad 是叶子节点的梯度累加器，通常是终点\n",
    "            if next_node is not None:\n",
    "                print_autograd_graph(next_node, level + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3acd5ac",
   "metadata": {},
   "source": [
    "### 测试计算图遍历"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cc0bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "# 构造一个稍微复杂的路径: z = (x * y) + x\n",
    "z = x * y\n",
    "out = z + x\n",
    "\n",
    "print(\"=== Autograd Graph Traversal ===\")\n",
    "print(f\"Root: {out.grad_fn}\")  # AddBackward\n",
    "print_autograd_graph(out.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaef0aa",
   "metadata": {},
   "source": [
    "### 关于 AccumulateGrad\n",
    "\n",
    "这种方法最能体现 PyTorch 动态图的本质：图就是一堆链式引用的 C++ 对象。\n",
    "\n",
    "你可以清晰地看到，因为 x 在计算中参与了两次（一次在乘法里，一次在加法里），所以 AccumulateGrad (对应 x) 在树中出现了两次。这解释了为什么梯度需要累加。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d82a7ea",
   "metadata": {},
   "source": [
    "## 使用 TensorBoard 可视化计算图\n",
    "\n",
    "定义一个简单的网络并使用 TensorBoard 记录图结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d98a0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8621a06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNet()\n",
    "dummy_input = torch.randn(1, 10)\n",
    "\n",
    "# 使用 SummaryWriter 记录图\n",
    "writer = SummaryWriter(\"runs/graph_visualization\")\n",
    "\n",
    "# add_graph 需要模型实例和样例输入，它会追踪一次 forward 过程\n",
    "writer.add_graph(model, dummy_input)\n",
    "writer.close()\n",
    "\n",
    "print(\"请在终端运行: tensorboard --logdir=runs\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
