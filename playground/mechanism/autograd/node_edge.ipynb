{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c98350b6",
   "metadata": {},
   "source": [
    "## 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e60a8826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d7c7c3",
   "metadata": {},
   "source": [
    "## 基本示例：理解 Node 和 Edge\n",
    "\n",
    "创建叶子节点并构建简单的计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af1d383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 创建叶子节点 (AccumulateGrad 节点的前身)\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "# 2. 执行前向计算，构建图\n",
    "# z = x * y  -> 产生 MulBackward0 Node\n",
    "# out = z + 1 -> 产生 AddBackward0 Node\n",
    "z = x * y\n",
    "out = z + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d38274",
   "metadata": {},
   "source": [
    "## 探索计算图结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23862e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Node (grad_fn): <AddBackward0 object at 0x111d20880>\n",
      "Input Edges for Output: ((<MulBackward0 object at 0x10a32a710>, 0), (None, 0))\n"
     ]
    }
   ],
   "source": [
    "print(f\"Output Node (grad_fn): {out.grad_fn}\")\n",
    "print(f\"Input Edges for Output: {out.grad_fn.next_functions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b793f447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Previous Node (z): <MulBackward0 object at 0x10a3337f0>\n",
      "Input Edges for z: ((<AccumulateGrad object at 0x10a2a02e0>, 0), (<AccumulateGrad object at 0x10a2a1b10>, 0))\n"
     ]
    }
   ],
   "source": [
    "# 深入一层，查看 z 的 Node\n",
    "z_node = out.grad_fn.next_functions[0][0]\n",
    "print(f\"\\nPrevious Node (z): {z_node}\")\n",
    "print(f\"Input Edges for z: {z_node.next_functions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e47ceae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Edges for x: ()\n",
      "Input Edges for y: ()\n",
      "Leaf Node (x): <AccumulateGrad object at 0x10a2a1d80>\n",
      "Leaf Node (y): <AccumulateGrad object at 0x10a2a1db0>\n"
     ]
    }
   ],
   "source": [
    "# 继续深入，查看 x 和 y 的 Node\n",
    "x_node = z_node.next_functions[0][0]\n",
    "y_node = z_node.next_functions[1][0]\n",
    "print(f\"Input Edges for x: {x_node.next_functions}\")\n",
    "print(f\"Input Edges for y: {y_node.next_functions}\")\n",
    "print(f\"Leaf Node (x): {x_node}\")\n",
    "print(f\"Leaf Node (y): {y_node}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c9f675",
   "metadata": {},
   "source": [
    "### 解释 Node 和 Edge 之间的关系\n",
    "\n",
    "- **Node**：表示一个具体的操作（如加法、乘法等），对应于计算图中的一个结点。\n",
    "- **Edge**：表示节点之间的数据流动关系，即一个节点的输出作为另一个节点的输入。\n",
    "\n",
    "在上面的例子中：\n",
    "- `out` 节点是一个 AddBackward 节点，表示执行了加法操作。\n",
    "- `out` 节点有两个输入边（Edges），分别指向 z 节点和一个常数 1（常数没有对应的 Node，因此是 None）。\n",
    "- `z` 节点是一个 MulBackward 节点，表示执行了乘法操作。\n",
    "- `z` 节点有两个输入边，分别指向 x 和 y 节点，这两个节点都是叶子节点（AccumulateGrad 节点）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb3a096",
   "metadata": {},
   "source": [
    "## 复杂计算图示例\n",
    "\n",
    "构建一个更复杂的计算图来展示多层结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4aa1df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Complex Output Node (d) grad_fn: <DivBackward0 object at 0x113a27520>\n",
      "Input Edges for d: ((<AddBackward0 object at 0x10a416f80>, 0), (<MulBackward0 object at 0x10a43c190>, 0))\n"
     ]
    }
   ],
   "source": [
    "# 新增叶子结点 a b c\n",
    "a = torch.tensor([4.0], requires_grad=True)\n",
    "b = torch.tensor([5.0], requires_grad=True)\n",
    "c = torch.tensor([6.0], requires_grad=True)\n",
    "\n",
    "# 构建更复杂的计算图\n",
    "d = (a + b + c) / (x * y)\n",
    "print(f\"\\nComplex Output Node (d) grad_fn: {d.grad_fn}\")\n",
    "print(f\"Input Edges for d: {d.grad_fn.next_functions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0feb2f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Addition Node in d: <AddBackward0 object at 0x10a3303a0>\n",
      "Input Edges for Addition Node: ((<AddBackward0 object at 0x10a332260>, 0), (<AccumulateGrad object at 0x10a333370>, 0))\n",
      "Left Input Edges for Addition Node: ((<AccumulateGrad object at 0x10a2a2ef0>, 0), (<AccumulateGrad object at 0x10a2a3400>, 0))\n",
      "Right Input Edges for Addition Node: ()\n",
      "\n",
      "Multiplication Node in d: <MulBackward0 object at 0x10a333940>\n",
      "Input Edges for Multiplication Node: ((<AccumulateGrad object at 0x10a2a1d80>, 0), (<AccumulateGrad object at 0x10a2a1db0>, 0))\n"
     ]
    }
   ],
   "source": [
    "# 查看 d 的计算图的更深层次结构\n",
    "add_node = d.grad_fn.next_functions[0][0]\n",
    "mul_node = d.grad_fn.next_functions[1][0]\n",
    "print(f\"\\nAddition Node in d: {add_node}\")\n",
    "print(f\"Input Edges for Addition Node: {add_node.next_functions}\")\n",
    "add_node_left = add_node.next_functions[0][0]\n",
    "add_node_right = add_node.next_functions[1][0]\n",
    "print(f\"Left Input Edges for Addition Node: {add_node_left.next_functions}\")\n",
    "print(f\"Right Input Edges for Addition Node: {add_node_right.next_functions}\")\n",
    "\n",
    "print(f\"\\nMultiplication Node in d: {mul_node}\")\n",
    "print(f\"Input Edges for Multiplication Node: {mul_node.next_functions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a6f096",
   "metadata": {},
   "source": [
    "## AccumulateGrad：叶子节点的梯度累加器\n",
    "\n",
    "AccumulateGrad 是叶子节点（变量）的梯度收集节点。\n",
    "\n",
    "### 为什么叫 \"Accumulate\" (累加)？\n",
    "\n",
    "在反向传播结束时，计算出的梯度需要存入变量的 `.grad` 属性中。如果一个变量在计算图中被使用了多次（例如 `y = x * x`），会有多条路径传回梯度。PyTorch 的机制是将这些梯度累加（Accumulate）到 `.grad` 中，而不是覆盖它。因此，这个节点的名字叫 AccumulateGrad，代表\"将梯度累加到叶子节点的 .grad 属性里\"。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195eaa13",
   "metadata": {},
   "source": [
    "## 梯度累加示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3ba90ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 - w.grad (Expected 2.0): 2.0\n"
     ]
    }
   ],
   "source": [
    "# 模拟一个共享权重 w\n",
    "w = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# 路径 1: Loss1 = w * 2 -> d(Loss1)/dw = 2\n",
    "loss1 = w * 2\n",
    "loss1.backward()\n",
    "print(f\"Step 1 - w.grad (Expected 2.0): {w.grad.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dcc38c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 - w.grad (Expected 2.0 + 3.0 = 5.0): 5.0\n"
     ]
    }
   ],
   "source": [
    "# 路径 2: Loss2 = w * 3 -> d(Loss2)/dw = 3\n",
    "# 注意：我们需要再次构建计算图，因为上一次已经被销毁\n",
    "loss2 = w * 3\n",
    "loss2.backward()\n",
    "\n",
    "# 关键点：此时 w.grad 不是 3.0，而是 2.0 + 3.0 = 5.0\n",
    "print(f\"Step 2 - w.grad (Expected 2.0 + 3.0 = 5.0): {w.grad.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6330df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 - After zero_grad: 0.0\n"
     ]
    }
   ],
   "source": [
    "# 手动清零 (模拟 optimizer.zero_grad())\n",
    "w.grad.zero_()\n",
    "print(f\"Step 3 - After zero_grad: {w.grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e7d7d7",
   "metadata": {},
   "source": [
    "### 重要结论\n",
    "\n",
    "当 `backward()` 触及叶子节点 w 时，它触发了 w 关联的 AccumulateGrad Node。这个 Node 执行的操作实质上是 `w.grad = w.grad + new_grad`。\n",
    "\n",
    "这就是为什么在训练循环中，必须在 `loss.backward()` 之前调用 `optimizer.zero_grad()`，否则梯度会跨越 batch 累积，导致参数更新方向错误。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
